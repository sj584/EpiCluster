{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Data_preprocessing/example_pyg_surface.pkl\", \"rb\") as f:\n",
    "    example_pyg_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c6878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from models.model import JointModel\n",
    "from models.KNN import KNN\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_start = time.time()\n",
    "epochs = 400\n",
    "batch_size = 8\n",
    "data = example_pyg_list\n",
    "\n",
    "k = 15\n",
    "self_w = 0.25\n",
    "directory = \"Training\"\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "kfold = 5\n",
    "X = data\n",
    "kf = KFold(n_splits=kfold) ## shuffle, random_state edited. not tested yet!\n",
    "kf.get_n_splits(X)\n",
    "print(kf)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "    \n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    model = JointModel(node1_feat=83, n_edge_feat=1, n_hidden_feat1=64, n_hidden_feat2=128, device=device, attention=True, n_layer1=4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-8)\n",
    "    \n",
    "    best_val_loss = 100000000\n",
    "    best_auc_roc = 0\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_auc_history = []\n",
    "    \n",
    "    # the dataset cross-validation setup\n",
    "    \n",
    "    # dataset looks like this (v, abbreviation for validation set)\n",
    "    # ex) ㅁㅁㅁ...ㅁㅁ, \n",
    "    # 1st fold    vㅁㅁ...ㅁㅁ\n",
    "    # 2nd fold    ㅁvㅁ...ㅁㅁ\n",
    "    # kth fold    ㅁㅁㅁ...ㅁv\n",
    "    \n",
    "    test_start, test_end = test_index[0], test_index[-1] \n",
    "    print(\"test_index :\", test_start,\"~\", test_end)\n",
    "    X_test = X[test_start:test_end+1]\n",
    "    print(\"len X_test :\", len(X_test))\n",
    "    X_train = list(set(X) - set(X_test))\n",
    "    print(\"len X_train :\", len(X_train))\n",
    "    train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        since = time.time()\n",
    "        train_loss_list = []\n",
    "        model.train()\n",
    "        model.cuda()\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            batch.to(device)\n",
    "            batch.y = batch.y.float()\n",
    "            knn_out_batch = torch.tensor([]).to(\"cuda\")\n",
    "            out = model(batch)\n",
    "            for idx in range(batch_size):\n",
    "                # since KNN is instance learning, batch learning is not feasible\n",
    "                # instead, we dissociate the batch and apply KNN algorithm iteratively\n",
    "                # then we restore the batch size by concatenation\n",
    "                try:\n",
    "                    regressor = KNN(k=k+1, include_self=True, weights=\"distance\", self_weight=self_w)\n",
    "                    regressor.train(batch[idx].coords[batch[idx].train_mask], out[batch.ptr[idx]:batch.ptr[idx+1]][batch[idx].train_mask])\n",
    "                    knn_out = regressor.predict(batch[idx].coords[batch[idx].train_mask])\n",
    "                    knn_out_batch = torch.concat((knn_out_batch, knn_out))\n",
    "                # the last batch may not have full batch size, in this case, just make the batch with size as (number of dataset % batch_size)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            loss = loss_fn(knn_out_batch.reshape(-1), batch.y[batch.train_mask]) # out[batch.train_mask].reshape(-1)\n",
    "            loss.backward()\n",
    "            train_loss_list.append(copy.deepcopy(loss.data.cpu().numpy()))\n",
    "\n",
    "            optimizer.step()\n",
    "        epoch_train_avg_loss = np.sum(np.array(train_loss_list))/len(train_loader) \n",
    "        train_loss_history.append(epoch_train_avg_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_list = []\n",
    "        val_auc_list = []\n",
    "        total_pred_label_list = []\n",
    "        total_true_label_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch.to(device)\n",
    "                batch.y = batch.y.float()\n",
    "                knn_out_batch = torch.tensor([]).to(\"cuda\")\n",
    "                out = model(batch)\n",
    "                for idx in range(batch_size):\n",
    "                # same as above\n",
    "                    try:\n",
    "                        regressor = KNN(k=k, include_self=True, weights=\"distance\", self_weight=self_w)\n",
    "                        regressor.train(batch[idx].coords[batch[idx].train_mask], out[batch.ptr[idx]:batch.ptr[idx+1]][batch[idx].train_mask])\n",
    "                        knn_out = regressor.predict(batch[idx].coords[batch[idx].train_mask])\n",
    "                        knn_out_batch = torch.concat((knn_out_batch, knn_out))\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                loss = loss_fn(knn_out_batch.reshape(-1), batch.y[batch.train_mask])\n",
    "\n",
    "                pred_label_list = []\n",
    "                for i in knn_out_batch:\n",
    "                    pred_label_list.append(i.cpu().numpy())\n",
    "\n",
    "                true_label_list = []\n",
    "                for j in batch.y[batch.train_mask]:\n",
    "                    true_label_list.append(j.cpu().numpy())\n",
    "\n",
    "                fpr, tpr, threshold = metrics.roc_curve(true_label_list, pred_label_list)\n",
    "                roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "                val_auc_list.append(copy.deepcopy(roc_auc))\n",
    "                val_loss_list.append(copy.deepcopy(loss.data.cpu().numpy()))\n",
    "\n",
    "\n",
    "        epoch_val_avg_loss = np.sum(np.array(val_loss_list))/len(test_loader) \n",
    "        val_loss_history.append(epoch_val_avg_loss)\n",
    "\n",
    "        epoch_val_auc_roc =  np.sum(np.array(val_auc_list))/len(test_loader)    \n",
    "        val_auc_history.append(epoch_val_avg_loss)\n",
    "\n",
    "        \n",
    "        if (epoch_val_avg_loss < best_val_loss) & (epoch_val_auc_roc > best_auc_roc): \n",
    "            best_epoch = epoch+1\n",
    "            best_val_loss = epoch_val_avg_loss\n",
    "            best_auc_roc = epoch_val_auc_roc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        end = time.time()\n",
    "        print(f'{epoch+1}th epoch,')\n",
    "        print(f'\\ttraining loss: {epoch_train_avg_loss:.5f}')\n",
    "        print(f'\\tval loss: {epoch_val_avg_loss:.5f}')\n",
    "        print(f'\\tval auc-roc: {epoch_val_auc_roc:.5f}')\n",
    "        print(f'\\tepoch time: {end-since:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "    save_dir = directory\n",
    "    import os\n",
    "    if os.path.isdir(save_dir):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(save_dir)\n",
    "    torch.save(best_model_wts, f'{save_dir}/Best_model_{best_epoch}_{fold_idx}.pt')\n",
    "    print('-'*10)\n",
    "    print('Train Finished.')\n",
    "    print(f'Training time: {time.time()-train_start:.2f}s')\n",
    "    print(f'The best epoch: {best_epoch}')\n",
    "    print(f'The best val loss: {best_val_loss}')\n",
    "    print(f\"The best auc-roc: {best_auc_roc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaaed01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[torch]",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
