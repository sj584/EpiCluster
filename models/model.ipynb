{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabaec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e_gcl_mask import E_GCL_mask\n",
    "import torch.nn as nn\n",
    "\n",
    "class EGNN_block(nn.Module):\n",
    "    def __init__(self, in_node_nf, in_edge_nf, hidden_nf, device='cpu', act_fn=nn.SiLU(), n_layers=4, coords_weight=1.0, attention=False, node_attr=1):\n",
    "        super(EGNN_block, self).__init__()\n",
    "        self.hidden_nf = hidden_nf\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "\n",
    "        ### Encoder\n",
    "        self.embedding = nn.Linear(in_node_nf, hidden_nf)\n",
    "        self.node_attr = node_attr\n",
    "        if node_attr:\n",
    "            n_node_attr = in_node_nf\n",
    "        else:\n",
    "            n_node_attr = 0\n",
    "        for i in range(0, n_layers):\n",
    "            self.add_module(\"gcl_%d\" % i, E_GCL_mask(self.hidden_nf, self.hidden_nf, self.hidden_nf, edges_in_d=in_edge_nf, nodes_attr_dim=n_node_attr, act_fn=act_fn, recurrent=True, coords_weight=coords_weight, attention=attention))\n",
    "\n",
    "        self.node_dec = nn.Sequential(nn.Linear(self.hidden_nf, self.hidden_nf),\n",
    "                                      act_fn,\n",
    "                                      nn.Linear(self.hidden_nf, self.hidden_nf))\n",
    "\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.hidden_nf, self.hidden_nf),\n",
    "                                       act_fn,\n",
    "                                       nn.Linear(self.hidden_nf, self.hidden_nf))\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, h0, x, edges, edge_attr, node_mask, edge_mask, n_nodes): # edge_attr -> node_attr ??\n",
    "        h = self.embedding(h0)\n",
    "        for i in range(0, self.n_layers):\n",
    "            if self.node_attr:\n",
    "                h, _, _ = self._modules[\"gcl_%d\" % i](h, edges, x, node_mask, edge_mask, edge_attr=edge_attr, node_attr=h0, n_nodes=n_nodes)\n",
    "            else:\n",
    "                h, _, _ = self._modules[\"gcl_%d\" % i](h, edges, x, node_mask, edge_mask, edge_attr=edge_attr,\n",
    "                                                      node_attr=None, n_nodes=n_nodes)\n",
    "\n",
    "        h = self.node_dec(h)\n",
    "        h = h * node_mask\n",
    "        h = h.view(n_nodes, self.hidden_nf)\n",
    "        \n",
    "        return h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f34420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_block(nn.Module):\n",
    "    def __init__(self, n_hidden_feat, out_feat):\n",
    "        super(FFNN_block, self).__init__()\n",
    "        self.fc1 = nn.Linear(1280, n_hidden_feat)\n",
    "        self.fc2 = nn.Linear(n_hidden_feat, out_feat)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)   # [num_nodes, 256]\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)   # [num_nodes, 128]\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, node1_feat, n_edge_feat, n_hidden_feat1, n_hidden_feat2, device, attention, n_layer1):\n",
    "        super(JointModel, self).__init__()\n",
    "        \n",
    "        self.struct = EGNN_block(node1_feat, n_edge_feat, n_hidden_feat1, n_hidden_feat2, attention=True, n_layers=n_layer1).to(device)\n",
    "        self.esm = FFNN_block().to(device)\n",
    "        self.last_dec = nn.Sequential(nn.Linear(n_hidden_feat1+n_hidden_feat2, n_hidden_feat1+n_hidden_feat2),  # n_hidden_feat1+n_hidden_feat2\n",
    "                                       nn.SiLU(),\n",
    "                                       nn.Linear(n_hidden_feat1+n_hidden_feat2, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        struct_feats = self.struct(x.node_attrs[:, :83], x.coords, x.edge_index, x.edge_attrs.reshape(-1, 1) , 1, 1, x.num_nodes)\n",
    "        esm_feats = self.esm(x.node_attrs[:, 83:]) \n",
    "        concat_feat = torch.concat((struct_feats, esm_feats), dim=1)\n",
    "        out = self.last_dec(concat_feat)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
